{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# See description above for what these variables represent\n",
    "big_loop_iterations = 10\n",
    "explainer_runs = 10\n",
    "thresholds = [30, 50]\n",
    "samples = [10]\n",
    "\n",
    "# Used to label the resulting files -\n",
    "date = \"quantile_alg_final\"\n",
    "\n",
    "# Set up directory for result files\n",
    "import os\n",
    "dir = f'./results_{date}'\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is connected  False\n",
      "Calculate subgraph ...\n",
      "Number of subgraphs:  118\n",
      "Size of subgraph:  2049\n",
      "Graph is connected  True\n",
      "##################\n",
      "# DATASET LOADED #\n",
      "##################\n",
      "\n",
      "Number of nodes: 2049\n",
      "Number of edges: 13588\n",
      "Number of modalities: 2\n"
     ]
    }
   ],
   "source": [
    "from GNNSubNet import GNNSubNet as gnn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# # Kidney data set  ------------------------- #\n",
    "loc   = \"../TCGA\"\n",
    "ppi   = f'{loc}/KIDNEY_RANDOM_PPI.txt'\n",
    "feats = [f'{loc}/KIDNEY_RANDOM_Methy_FEATURES.txt', f'{loc}/KIDNEY_RANDOM_mRNA_FEATURES.txt']\n",
    "targ  = f'{loc}/KIDNEY_RANDOM_TARGET.txt'\n",
    "\n",
    "# # Synthetic data set  ------------------------- #\n",
    "# loc   = \"../GNNSubNet/datasets/synthetic/\"\n",
    "# ppi   = f'{loc}/NETWORK_synthetic.txt'\n",
    "# feats = [f'{loc}/FEATURES_synthetic.txt']\n",
    "# targ  = f'{loc}/TARGET_synthetic.txt'\n",
    "\n",
    "# Read in the synthetic data\n",
    "g = gnn.GNNSubNet(loc, ppi, feats, targ, normalize=False)\n",
    "\n",
    "# Get some general information about the data dimension\n",
    "g.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_BAGEL_scores(loc, ppi, feats, targ, gnn_subnet, quantile_aggregation=False, quantile=0.5):\n",
    "    model_info = []\n",
    "    fidelity = []\n",
    "    validity_plus = []\n",
    "    validity_minus = []\n",
    "    validity_plus_matrix = []\n",
    "    validity_minus_matrix = []\n",
    "    sparsity = []\n",
    "\n",
    "    for i in range(9, big_loop_iterations):\n",
    "        print(i)\n",
    "        g = gnn.GNNSubNet(loc, ppi, feats, targ, normalize=False)\n",
    "        g.train()\n",
    "\n",
    "        # Check the performance of the classifier\n",
    "        accuracy = g.accuracy\n",
    "\n",
    "        # Run the explainer the desired number of times\n",
    "        g.explain(explainer_runs, gnn_subnet=gnn_subnet, quantile_aggregation=quantile_aggregation, quantile=quantile)\n",
    "\n",
    "        # Fidelity\n",
    "        f = g.evaluate_RDT_fidelity_soft()\n",
    "        fidelity.append([i, accuracy, np.mean(f)])\n",
    "        # Save mean fidelity for each sample for further analysis\n",
    "        filename = f\"results_{date}/{i}_fidelities.csv\"\n",
    "        np.savetxt(filename, fidelity, delimiter=',', fmt ='% s')\n",
    "\n",
    "        # Sparsity\n",
    "        sparsities = g.evaluate_sparsity()\n",
    "        # Save raw results in case needed for further analysis\n",
    "        filename = f\"results_{date}/{i}_sparsities.csv\"\n",
    "        np.savetxt(filename, sparsities, delimiter=',', fmt ='% s')\n",
    "        # Save mean sparsity to list to create processed table\n",
    "        sparsity.append([i, accuracy, np.mean(sparsities)])\n",
    "\n",
    "        # Validity at varying thresholds\n",
    "        for t in thresholds:\n",
    "            v_plus, v_minus, mat_plus, mat_minus = g.evaluate_validity(threshold=t, confusion_matrix=True)\n",
    "            validity_plus.append([i, accuracy, t, v_plus])\n",
    "            validity_minus.append([i, accuracy, t, v_minus])\n",
    "            validity_plus_matrix.append([i, accuracy, t, mat_plus[0,0], mat_plus[0,1], mat_plus[1,0], mat_plus[1,1]])\n",
    "            validity_minus_matrix.append([i, accuracy, t, mat_minus[0,0], mat_minus[0,1], mat_minus[1,0], mat_minus[1,1]])\n",
    "\n",
    "        filename = f\"results_{date}/{i}_validity_plus.csv\"\n",
    "        np.savetxt(filename, validity_plus, delimiter=',', fmt ='% s')\n",
    "\n",
    "        filename = f\"results_{date}/{i}_validity_minus.csv\"\n",
    "        np.savetxt(filename, validity_minus, delimiter=',', fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_quantile_alg_final\n"
     ]
    }
   ],
   "source": [
    "print(f\"results_{date}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Graph is connected  False\n",
      "Calculate subgraph ...\n",
      "Number of subgraphs:  118\n",
      "Size of subgraph:  2049\n",
      "Graph is connected  True\n",
      "##################\n",
      "# DATASET LOADED #\n",
      "##################\n",
      "graphcnn for training ...\n",
      "Graphs class 0: 200, Graphs class 1: 306\n",
      "Length of balanced dataset list: 400\n",
      "Train graph class 0: 162, train graph class 1: 158\n",
      "Validation graph class 0: 38, validation graph class 1: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:00<?, ?batch/s]C:\\Users\\elena\\DELFT\\assignments\\RP\\GNN-SubNet_modified\\GNNSubNet\\graphcnn.py:134: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:623.)\n",
      "  graph_pool = torch.sparse.FloatTensor(idx, elem, torch.Size([len(batch_graph), start_idx[-1]]))\n",
      "100%|██████████| 35/35 [00:08<00:00,  4.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 2190.6757\n",
      "Train Acc 0.5156\n",
      "Epoch 0, val_loss 24.9896\n",
      "Saving best model with validation loss 24.989639282226562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  3.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 332.4395\n",
      "Train Acc 0.5062\n",
      "Epoch 1, val_loss 185.1322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:10<00:00,  3.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss 56.7439\n",
      "Train Acc 0.4938\n",
      "Epoch 2, val_loss 236.2896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:10<00:00,  3.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss 27.2859\n",
      "Train Acc 0.5437\n",
      "Epoch 3, val_loss 14.2863\n",
      "Saving best model with validation loss 14.2862548828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:08<00:00,  4.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss 24.2290\n",
      "Train Acc 0.6031\n",
      "Epoch 4, val_loss 8.3385\n",
      "Saving best model with validation loss 8.338519096374512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:09<00:00,  3.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss 29.9741\n",
      "Train Acc 0.6312\n",
      "Epoch 5, val_loss 15.2460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:12<00:00,  2.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss 25.2902\n",
      "Train Acc 0.6281\n",
      "Epoch 6, val_loss 17.3517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:12<00:00,  2.87batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss 19.7972\n",
      "Train Acc 0.5062\n",
      "Epoch 7, val_loss 21.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  3.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss 35.4303\n",
      "Train Acc 0.6937\n",
      "Epoch 8, val_loss 4.8031\n",
      "Saving best model with validation loss 4.803082466125488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  3.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss 25.2290\n",
      "Train Acc 0.6156\n",
      "Epoch 9, val_loss 9.2127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  3.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss 19.4481\n",
      "Train Acc 0.7781\n",
      "Epoch 10, val_loss 3.3512\n",
      "Saving best model with validation loss 3.351222276687622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:12<00:00,  2.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss 22.6717\n",
      "Train Acc 0.7625\n",
      "Epoch 11, val_loss 4.4268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:12<00:00,  2.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, loss 17.1809\n",
      "Train Acc 0.5062\n",
      "Epoch 12, val_loss 26.8993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:08<00:00,  4.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, loss 16.5628\n",
      "Train Acc 0.5844\n",
      "Epoch 13, val_loss 22.7078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, loss 27.8319\n",
      "Train Acc 0.6875\n",
      "Epoch 14, val_loss 21.8803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, loss 16.1926\n",
      "Train Acc 0.7937\n",
      "Epoch 15, val_loss 3.4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, loss 12.7976\n",
      "Train Acc 0.8531\n",
      "Epoch 16, val_loss 4.1890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, loss 12.1166\n",
      "Train Acc 0.5312\n",
      "Epoch 17, val_loss 10.3126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, loss 15.5479\n",
      "Train Acc 0.4969\n",
      "Epoch 18, val_loss 29.1389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, loss 19.2215\n",
      "Train Acc 0.7750\n",
      "Epoch 19, val_loss 4.0681\n",
      "\n",
      "Confusion matrix (Validation set):\n",
      "\n",
      "[[25 13]\n",
      " [ 9 33]]\n",
      "Validation accuracy: 72.5%\n",
      "Validation loss 3.351222276687622\n",
      "\n",
      "------- Run the Explainer -------\n",
      "\n",
      "GNN-SubNet: False\n",
      "Explainer::Iteration 1 of 10\n",
      "Explainer::Iteration 2 of 10\n",
      "Explainer::Iteration 3 of 10\n",
      "Explainer::Iteration 4 of 10\n",
      "Explainer::Iteration 5 of 10\n",
      "Explainer::Iteration 6 of 10\n",
      "Explainer::Iteration 7 of 10\n",
      "Explainer::Iteration 8 of 10\n",
      "Explainer::Iteration 9 of 10\n",
      "Explainer::Iteration 10 of 10\n",
      "Evaluating RDT-fidelity using 80 graphs from the test dataset\n",
      "Evaluating 1 out of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elena\\DELFT\\assignments\\RP\\GNN-SubNet_modified\\GNNSubNet\\GNNSubNet.py:1715: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_graph = Data(x=torch.tensor(perturbed_features).float().to(\"cpu\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 2 out of 80\n",
      "Evaluating 3 out of 80\n",
      "Evaluating 4 out of 80\n",
      "Evaluating 5 out of 80\n",
      "Evaluating 6 out of 80\n",
      "Evaluating 7 out of 80\n",
      "Evaluating 8 out of 80\n",
      "Evaluating 9 out of 80\n",
      "Evaluating 10 out of 80\n",
      "Evaluating 11 out of 80\n",
      "Evaluating 12 out of 80\n",
      "Evaluating 13 out of 80\n",
      "Evaluating 14 out of 80\n",
      "Evaluating 15 out of 80\n",
      "Evaluating 16 out of 80\n",
      "Evaluating 17 out of 80\n",
      "Evaluating 18 out of 80\n",
      "Evaluating 19 out of 80\n",
      "Evaluating 20 out of 80\n",
      "Evaluating 21 out of 80\n",
      "Evaluating 22 out of 80\n",
      "Evaluating 23 out of 80\n",
      "Evaluating 24 out of 80\n",
      "Evaluating 25 out of 80\n",
      "Evaluating 26 out of 80\n",
      "Evaluating 27 out of 80\n",
      "Evaluating 28 out of 80\n",
      "Evaluating 29 out of 80\n",
      "Evaluating 30 out of 80\n",
      "Evaluating 31 out of 80\n",
      "Evaluating 32 out of 80\n",
      "Evaluating 33 out of 80\n",
      "Evaluating 34 out of 80\n",
      "Evaluating 35 out of 80\n",
      "Evaluating 36 out of 80\n",
      "Evaluating 37 out of 80\n",
      "Evaluating 38 out of 80\n",
      "Evaluating 39 out of 80\n",
      "Evaluating 40 out of 80\n",
      "Evaluating 41 out of 80\n",
      "Evaluating 42 out of 80\n",
      "Evaluating 43 out of 80\n",
      "Evaluating 44 out of 80\n",
      "Evaluating 45 out of 80\n",
      "Evaluating 46 out of 80\n",
      "Evaluating 47 out of 80\n",
      "Evaluating 48 out of 80\n",
      "Evaluating 49 out of 80\n",
      "Evaluating 50 out of 80\n",
      "Evaluating 51 out of 80\n",
      "Evaluating 52 out of 80\n",
      "Evaluating 53 out of 80\n",
      "Evaluating 54 out of 80\n",
      "Evaluating 55 out of 80\n",
      "Evaluating 56 out of 80\n",
      "Evaluating 57 out of 80\n",
      "Evaluating 58 out of 80\n",
      "Evaluating 59 out of 80\n",
      "Evaluating 60 out of 80\n",
      "Evaluating 61 out of 80\n",
      "Evaluating 62 out of 80\n",
      "Evaluating 63 out of 80\n",
      "Evaluating 64 out of 80\n",
      "Evaluating 65 out of 80\n",
      "Evaluating 66 out of 80\n",
      "Evaluating 67 out of 80\n",
      "Evaluating 68 out of 80\n",
      "Evaluating 69 out of 80\n",
      "Evaluating 70 out of 80\n",
      "Evaluating 71 out of 80\n",
      "Evaluating 72 out of 80\n",
      "Evaluating 73 out of 80\n",
      "Evaluating 74 out of 80\n",
      "Evaluating 75 out of 80\n",
      "Evaluating 76 out of 80\n",
      "Evaluating 77 out of 80\n",
      "Evaluating 78 out of 80\n",
      "Evaluating 79 out of 80\n",
      "Evaluating 80 out of 80\n",
      "Evaluating sparsity on 10 node masks\n",
      "Evaluating Validity+ and Validity- using 80 graphs from the test dataset\n",
      "Evaluating 1 out of 80\n",
      "Evaluating 2 out of 80\n",
      "Evaluating 3 out of 80\n",
      "Evaluating 4 out of 80\n",
      "Evaluating 5 out of 80\n",
      "Evaluating 6 out of 80\n",
      "Evaluating 7 out of 80\n",
      "Evaluating 8 out of 80\n",
      "Evaluating 9 out of 80\n",
      "Evaluating 10 out of 80\n",
      "Evaluating 11 out of 80\n",
      "Evaluating 12 out of 80\n",
      "Evaluating 13 out of 80\n",
      "Evaluating 14 out of 80\n",
      "Evaluating 15 out of 80\n",
      "Evaluating 16 out of 80\n",
      "Evaluating 17 out of 80\n",
      "Evaluating 18 out of 80\n",
      "Evaluating 19 out of 80\n",
      "Evaluating 20 out of 80\n",
      "Evaluating 21 out of 80\n",
      "Evaluating 22 out of 80\n",
      "Evaluating 23 out of 80\n",
      "Evaluating 24 out of 80\n",
      "Evaluating 25 out of 80\n",
      "Evaluating 26 out of 80\n",
      "Evaluating 27 out of 80\n",
      "Evaluating 28 out of 80\n",
      "Evaluating 29 out of 80\n",
      "Evaluating 30 out of 80\n",
      "Evaluating 31 out of 80\n",
      "Evaluating 32 out of 80\n",
      "Evaluating 33 out of 80\n",
      "Evaluating 34 out of 80\n",
      "Evaluating 35 out of 80\n",
      "Evaluating 36 out of 80\n",
      "Evaluating 37 out of 80\n",
      "Evaluating 38 out of 80\n",
      "Evaluating 39 out of 80\n",
      "Evaluating 40 out of 80\n",
      "Evaluating 41 out of 80\n",
      "Evaluating 42 out of 80\n",
      "Evaluating 43 out of 80\n",
      "Evaluating 44 out of 80\n",
      "Evaluating 45 out of 80\n",
      "Evaluating 46 out of 80\n",
      "Evaluating 47 out of 80\n",
      "Evaluating 48 out of 80\n",
      "Evaluating 49 out of 80\n",
      "Evaluating 50 out of 80\n",
      "Evaluating 51 out of 80\n",
      "Evaluating 52 out of 80\n",
      "Evaluating 53 out of 80\n",
      "Evaluating 54 out of 80\n",
      "Evaluating 55 out of 80\n",
      "Evaluating 56 out of 80\n",
      "Evaluating 57 out of 80\n",
      "Evaluating 58 out of 80\n",
      "Evaluating 59 out of 80\n",
      "Evaluating 60 out of 80\n",
      "Evaluating 61 out of 80\n",
      "Evaluating 62 out of 80\n",
      "Evaluating 63 out of 80\n",
      "Evaluating 64 out of 80\n",
      "Evaluating 65 out of 80\n",
      "Evaluating 66 out of 80\n",
      "Evaluating 67 out of 80\n",
      "Evaluating 68 out of 80\n",
      "Evaluating 69 out of 80\n",
      "Evaluating 70 out of 80\n",
      "Evaluating 71 out of 80\n",
      "Evaluating 72 out of 80\n",
      "Evaluating 73 out of 80\n",
      "Evaluating 74 out of 80\n",
      "Evaluating 75 out of 80\n",
      "Evaluating 76 out of 80\n",
      "Evaluating 77 out of 80\n",
      "Evaluating 78 out of 80\n",
      "Evaluating 79 out of 80\n",
      "Evaluating 80 out of 80\n",
      "Evaluating Validity+ and Validity- using 80 graphs from the test dataset\n",
      "Evaluating 1 out of 80\n",
      "Evaluating 2 out of 80\n",
      "Evaluating 3 out of 80\n",
      "Evaluating 4 out of 80\n",
      "Evaluating 5 out of 80\n",
      "Evaluating 6 out of 80\n",
      "Evaluating 7 out of 80\n",
      "Evaluating 8 out of 80\n",
      "Evaluating 9 out of 80\n",
      "Evaluating 10 out of 80\n",
      "Evaluating 11 out of 80\n",
      "Evaluating 12 out of 80\n",
      "Evaluating 13 out of 80\n",
      "Evaluating 14 out of 80\n",
      "Evaluating 15 out of 80\n",
      "Evaluating 16 out of 80\n",
      "Evaluating 17 out of 80\n",
      "Evaluating 18 out of 80\n",
      "Evaluating 19 out of 80\n",
      "Evaluating 20 out of 80\n",
      "Evaluating 21 out of 80\n",
      "Evaluating 22 out of 80\n",
      "Evaluating 23 out of 80\n",
      "Evaluating 24 out of 80\n",
      "Evaluating 25 out of 80\n",
      "Evaluating 26 out of 80\n",
      "Evaluating 27 out of 80\n",
      "Evaluating 28 out of 80\n",
      "Evaluating 29 out of 80\n",
      "Evaluating 30 out of 80\n",
      "Evaluating 31 out of 80\n",
      "Evaluating 32 out of 80\n",
      "Evaluating 33 out of 80\n",
      "Evaluating 34 out of 80\n",
      "Evaluating 35 out of 80\n",
      "Evaluating 36 out of 80\n",
      "Evaluating 37 out of 80\n",
      "Evaluating 38 out of 80\n",
      "Evaluating 39 out of 80\n",
      "Evaluating 40 out of 80\n",
      "Evaluating 41 out of 80\n",
      "Evaluating 42 out of 80\n",
      "Evaluating 43 out of 80\n",
      "Evaluating 44 out of 80\n",
      "Evaluating 45 out of 80\n",
      "Evaluating 46 out of 80\n",
      "Evaluating 47 out of 80\n",
      "Evaluating 48 out of 80\n",
      "Evaluating 49 out of 80\n",
      "Evaluating 50 out of 80\n",
      "Evaluating 51 out of 80\n",
      "Evaluating 52 out of 80\n",
      "Evaluating 53 out of 80\n",
      "Evaluating 54 out of 80\n",
      "Evaluating 55 out of 80\n",
      "Evaluating 56 out of 80\n",
      "Evaluating 57 out of 80\n",
      "Evaluating 58 out of 80\n",
      "Evaluating 59 out of 80\n",
      "Evaluating 60 out of 80\n",
      "Evaluating 61 out of 80\n",
      "Evaluating 62 out of 80\n",
      "Evaluating 63 out of 80\n",
      "Evaluating 64 out of 80\n",
      "Evaluating 65 out of 80\n",
      "Evaluating 66 out of 80\n",
      "Evaluating 67 out of 80\n",
      "Evaluating 68 out of 80\n",
      "Evaluating 69 out of 80\n",
      "Evaluating 70 out of 80\n",
      "Evaluating 71 out of 80\n",
      "Evaluating 72 out of 80\n",
      "Evaluating 73 out of 80\n",
      "Evaluating 74 out of 80\n",
      "Evaluating 75 out of 80\n",
      "Evaluating 76 out of 80\n",
      "Evaluating 77 out of 80\n",
      "Evaluating 78 out of 80\n",
      "Evaluating 79 out of 80\n",
      "Evaluating 80 out of 80\n"
     ]
    }
   ],
   "source": [
    "# Runs the experiment for a single explainer\n",
    "# Toggle the gnn_subnet parameter for the desired explainer\n",
    "obtain_BAGEL_scores(loc, ppi, feats, targ, gnn_subnet=False, quantile_aggregation=True, quantile=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
